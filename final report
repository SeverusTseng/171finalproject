%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Example: Project Report
%
% Source: http://www.howtotex.com
%
% Feel free to distribute this example, but please keep the referral
% to howtotex.com
% Date: March 2011 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
% If you're new to LaTeX, the wikibook is a great place to start:
% http://en.wikibooks.org/wiki/LaTeX
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Edit the title below to update the display in My Documents
%\title{Project Report}
%
%%% Preamble
\documentclass[paper=a4, fontsize=11pt, reqno]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{fourier}
\usepackage{times}
\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm,color} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}


%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule
\newcommand{\red}[1]{\textcolor{red}{#1}}

\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{University of California, Davis\\
        ECS 171 Machine Learning Final Project} \\ [25pt]
		\horrule{0.5pt} \\[0.4cm]
		\huge Loan Default Prediction \\
		\horrule{2pt} \\[2in]
}
\author{
		\normalfont 								%\normalsize
        Zongjian Fan\quad 915239513\\
        Huian Wang\quad \\
        Ruolan Zeng\quad 915536177\\
        Bohan Zhou\quad 998383636\\[2in]
		}
\date{\today}
\addbibresource{ECS171.bib}


%%% Begin document
\begin{document}
\maketitle
\newpage
%\section{Team Information}

% \begin{table}[!htbp]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% \multicolumn{2}{|c|}{Team Name}\\
% \hline
% Name&Kerbos Id\\
% \hline
% Zongjian Fan& 915239513\\
% \hline
% Huian Wang& 000\\
% \hline
% Ruolan Zeng& 915536177\\
% \hline
% Legal High& 000\\
% \hline
% \end{tabular}
% \end{table} 

\section{Problem Description}
We are given a training data with 50,000 observations. For each observation, it was recorded by 778 features and a loss. However f11, f12, f462, f463, f473, f474, f602, f603, f605 are abandoned, thus totally 769 valid features. In case of a default, the loss was measured among 0 and 100. We are required to think of a way to predict the loss for testing data with 55,470 observations.
\red{Note that some variables may be categorical (e.g. f776 and f777).}
\section{Data Analysis}
\subsection{Missing data}
We first collect features with more than 5\% of missing observations and delete those features in both traning and testing data.
\begin{table}[htbp]
\begin{minipage}[b]{0.45\linewidth}
\centering
\begin{tabular}{|c|c|}
\hline
Feature&Percentage(\%)\\
\hline
652& 17.9\\
\hline
653& 17.9\\
\hline
156& 17.7\\
\hline
157& 17.7\\
\hline
166& 17.5\\
\hline
167& 17.5\\
\hline
608& 17.4\\
\hline
609& 17.4\\
\hline
327& 17.2\\
\hline
328& 17.2\\
\hline
\end{tabular}
\caption{The first column is the index of features, the second column is the percentage of missing data.}
\end{minipage}\hfill
\begin{minipage}[b]{0.4\linewidth}
\centering
\includegraphics[height=60mm]{NANstat.png}
\captionof{figure}{The yellow bar are features with less than 2500 missing data, the blue bar are features with greater than 2500 missing data.}
\label{fig:image}
\end{minipage}
\end{table}

\subsection{Feature selection}
After dealing with those missing data and deleting those features with more than 5% missing data, we still have 726 features.
We don't need to keep all the features since many of them are highly related or even duplicated, which could lead to overfitting. Thereore, some strategies needed to select the features.\par
First of all, we removed all duplicated features. To complete that, if two features have more than 30000 corresponding same values, we regarded this pair as duplicated, and only kept one of them.\par
Then, we searched for all pairs of highly correlated features and kept there difference as a feature, which consisted a new feature set. In this step, we first chose a threshold 0.996, but after that only around 100 features were kept after all the selection processes, which was too less in our opinion. 
And thus we lowered the threshold to 0.993, and this time there were around 200 features in the last, and the predicting result was indeed improved by this, which suggested that only 100 features could lead to kind of underfitting.\par
Finally, we searched for all pairs of highly correlated features in the new feature set again, and removed all highly correlated features (correlation coefficient > 0:99). After this cleaning step, we obtained our final feature set for further analysis and training.

\subsection{Correlation matrix (heatmap)}
\subsection{Loss correlation matrix (zoomed heatmap)}
\subsection{Scatter plots between Loss and correlated features}
\subsection{Normality}

\section{Training Models}
\subsection{Classification}
At first we tried to predict where a loan is default or not, which is a classification problem. Therefore the new target to predict is given by:
$$ \textit{target}=\left\{
\begin{array}{rcl}
1       &      & \textit{loss} > 0\\
0     &      & \textit{loss} < 0
\end{array} \right. $$
For this task we used the complete feature set generated by our feature selection strategies. 
The classification itself was done by GradientBoostingClassifier in \testit{sklearn} package. 
And the parameters were setted as following: \textit{n_estimators} = 3000, \textit{max_depth} = 9. 
It is always not the best choice to choose the probability 0.5 as the cutoff of being predicted as the default loan.
After trying with different numbers and referring to some previous work, we choose the probability 0.55 to be the cutoff. 
Namely, when the default probability is less than 0.55, the loan is regarded as no default.
Otherwise, the loan is treated as the default loan, and would be processed by the regression phase.
\subsection{Regression}
After predicting whether the loans are default or not in testing data using gbm classifer mentioned above, we tried to predict the loss of all default loan.
Owing to the long tail distribution of the loss, it is a proper way to use $log(loss)$.
The feature set for the loss prediction consisted of all features from our selection process. 
The loss prediction was done by the combination of GBM regression and Support vector regression from \testit{sklearn} package, with weights of 0.6 and 0.4, separately.
The settings were as following: \textit{n_estimators} = 1300, \textit{max_depth} = 4, \textit{subsample} = 0.5, \textit{learning_rate} = 0.05 for GBM regression;
and \textit{C} = 16, \textit{kernel} = 'rbf', \textit{gamma} = 0.000122. 
And we used loss = 0 for all other cases.

\section{Code Description}
All our code is written by python, and consists of two files: \testbf{FeatureSelction.py} and \testbf{Predict.py}.
\testbf{FeatureSelction.py} contains all the functions for converting data, selecting features and generating data files for training.
And \testbf{Predict.py} uses all the data files generated by \testbf{FeatureSelction.py} to train and predict the results.
To generate the solution:
1. Run \testbf{FeatureSelction.py}.
2. Run \testbf{Predict.py} in the same directory.
3. Copy the predicted loss data to the sample submission files.

%%% End document
\nocite{*}
\printbibliography
\end{document}
